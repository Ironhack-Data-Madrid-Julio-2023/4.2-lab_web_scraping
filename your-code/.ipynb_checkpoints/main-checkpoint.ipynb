{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yiming Cui\n",
      "Stephen Celis\n",
      "Nikita Sobolev\n",
      "Jeff Dickey\n",
      "Suyeol Jeon\n",
      "Boni García\n",
      "Adeeb Shihadeh\n",
      "Bo-Yi Wu\n",
      "Abdullah Atta\n",
      "Dominic Farolino\n",
      "Jan-Erik Rediger\n",
      "Ismail Pelaseyed\n",
      "Steve Macenski\n",
      "Philipp Schmid\n",
      "Alessandro Ros\n",
      "Vladimir Mihailenco\n",
      "dennis zhuang\n",
      "Cameron Dutro\n",
      "MichaIng\n",
      "二货爱吃白萝卜\n",
      "Mattt\n",
      "Justin Clift\n",
      "Laurent Mazare\n",
      "Steven Tey\n",
      "Ha Thach\n"
     ]
    }
   ],
   "source": [
    "# Realizar la solicitud HTTP GET\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa (código de respuesta 200)\n",
    "if response.status_code == 200:\n",
    "    # Obtener el contenido HTML de la respuesta\n",
    "    html_content = response.text\n",
    "\n",
    "    # Analizar el contenido HTML con BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Encontrar todos los elementos con la clase \"h1\" que contienen los nombres de los desarrolladores (son elementos text)\n",
    "    developer_names = soup.find_all('h1', class_='h3 lh-condensed')\n",
    "\n",
    "    # Imprimir los nombres de los desarrolladores\n",
    "    for name in developer_names:\n",
    "        print(name.text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yiming Cui', 'Stephen Celis', 'Nikita Sobolev', 'Jeff Dickey', 'Suyeol Jeon', 'Boni García', 'Adeeb Shihadeh', 'Bo-Yi Wu', 'Abdullah Atta', 'Dominic Farolino', 'Jan-Erik Rediger', 'Ismail Pelaseyed', 'Steve Macenski', 'Philipp Schmid', 'Alessandro Ros', 'Vladimir Mihailenco', 'dennis zhuang', 'Cameron Dutro', 'MichaIng', '二货爱吃白萝卜', 'Mattt', 'Justin Clift', 'Laurent Mazare', 'Steven Tey', 'Ha Thach']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/trending/developers'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    developer_names = []\n",
    "    names_html_elements = soup.find_all('h1', class_='h3 lh-condensed')\n",
    "\n",
    "    for element in names_html_elements:\n",
    "        name = element.text.strip().replace('\\n', '')\n",
    "        developer_names.append(name)\n",
    "\n",
    "    print(developer_names) #estos nombres no tienen etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yiming Cui', 'Stephen Celis', 'Nikita Sobolev', 'Jeff Dickey', 'Suyeol Jeon', 'Boni García', 'Adeeb Shihadeh', 'Bo-Yi Wu', 'Abdullah Atta', 'Dominic Farolino', 'Jan-Erik Rediger', 'Ismail Pelaseyed', 'Steve Macenski', 'Philipp Schmid', 'Alessandro Ros', 'Vladimir Mihailenco', 'dennis zhuang', 'Cameron Dutro', 'MichaIng', '二货爱吃白萝卜', 'Mattt', 'Justin Clift', 'Laurent Mazare', 'Steven Tey', 'Ha Thach']\n"
     ]
    }
   ],
   "source": [
    "html = requests.get(url)\n",
    "\n",
    "if html.status_code == 200:\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "    # Encuentra todos los elementos que contienen los nombres de los repositorios\n",
    "    html_elements = soup.find_all('h1', class_='h3 lh-condensed')\n",
    "    repo_names = []   \n",
    "    for element in html_elements:\n",
    "        name = element.text.strip().replace('\\n', '')\n",
    "        repo_names.append(name)\n",
    "    print(repo_names)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/static/images/icons/wikipedia.png\n",
      "/static/images/mobile/copyright/wikipedia-wordmark-en.svg\n",
      "/static/images/mobile/copyright/wikipedia-tagline-en.svg\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/220px-Steamboat-willie.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/30px-Commons-logo.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/38px-Wikisource-logo.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/34px-Wikiquote-logo.svg.png\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/15px-Magic_Kingdom_castle.jpg\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/14px-Commons-logo.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/16px-Wikiquote-logo.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/18px-Wikisource-logo.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/21px-Wikidata-logo.svg.png\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
      "//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\n",
      "/static/images/footer/wikimedia-button.png\n",
      "/static/images/footer/poweredby_mediawiki_88x31.png\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html = requests.get(url)\n",
    "\n",
    "if html.status_code == 200:\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "    # Encuentra todos los elementos <img=imagen> en la página\n",
    "    image_elements = soup.find_all('img')\n",
    "\n",
    "    # Extrae la URL de cada imagen\n",
    "    image_links = [element['src'] for element in image_elements]\n",
    "\n",
    "    # Imprime las URLs de las imágenes\n",
    "    for link in image_links:\n",
    "        print(link)\n",
    "else:\n",
    "    print('Error al obtener la página:', html.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Main_Page\n",
      "/wiki/Wikipedia:Contents\n",
      "/wiki/Portal:Current_events\n",
      "/wiki/Special:Random\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Help:Contents\n",
      "/wiki/Help:Introduction\n",
      "/wiki/Wikipedia:Community_portal\n",
      "/wiki/Special:RecentChanges\n",
      "/wiki/Wikipedia:File_upload_wizard\n",
      "/wiki/Main_Page\n",
      "/wiki/Special:Search\n",
      "/wiki/Help:Introduction\n",
      "/wiki/Special:MyContributions\n",
      "/wiki/Special:MyTalk\n",
      "/wiki/Python\n",
      "/wiki/Talk:Python\n",
      "/wiki/Python\n",
      "/wiki/Python\n",
      "/wiki/Special:WhatLinksHere/Python\n",
      "/wiki/Special:RecentChangesLinked/Python\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:SpecialPages\n",
      "/wiki/Pythonidae\n",
      "/wiki/Python_(genus)\n",
      "/wiki/Python_(mythology)\n",
      "/wiki/Python_(programming_language)\n",
      "/wiki/CMU_Common_Lisp\n",
      "/wiki/PERQ#PERQ_3\n",
      "/wiki/Python_of_Aenus\n",
      "/wiki/Python_(painter)\n",
      "/wiki/Python_of_Byzantium\n",
      "/wiki/Python_of_Catana\n",
      "/wiki/Python_Anghelo\n",
      "/wiki/Python_(Efteling)\n",
      "/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "/wiki/Python_(automobile_maker)\n",
      "/wiki/Python_(Ford_prototype)\n",
      "/wiki/Python_(missile)\n",
      "/wiki/Python_(nuclear_primary)\n",
      "/wiki/Colt_Python\n",
      "/wiki/Python_(codename)\n",
      "/wiki/Python_(film)\n",
      "/wiki/Monty_Python\n",
      "/wiki/Python_(Monty)_Pictures\n",
      "/wiki/Timon_of_Phlius\n",
      "/wiki/Pyton\n",
      "/wiki/Pithon\n",
      "/wiki/File:Disambig_gray.svg\n",
      "/wiki/Help:Disambiguation\n",
      "/wiki/Help:Category\n",
      "/wiki/Category:Disambiguation_pages\n",
      "/wiki/Category:Human_name_disambiguation_pages\n",
      "/wiki/Category:Disambiguation_pages_with_given-name-holder_lists\n",
      "/wiki/Category:Short_description_is_different_from_Wikidata\n",
      "/wiki/Category:All_article_disambiguation_pages\n",
      "/wiki/Category:All_disambiguation_pages\n",
      "/wiki/Category:Animal_common_name_disambiguation_pages\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:General_disclaimer\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Python'\n",
    "html = requests.get(url)\n",
    "\n",
    "if html.status_code == 200:\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "    # Cada elemento <a> contiene un atributo href que especifica la URL de destino del enlace...\n",
    "    enlace_elements = soup.find_all('a')\n",
    "\n",
    "    # Extrae el atributo href de cada <a> para obtener la URL del enlace\n",
    "    enlaces = []\n",
    "    for enlace in enlace_elements:\n",
    "        if 'href' in enlace.attrs:\n",
    "            enlaces.append(enlace['href'])\n",
    "\n",
    "    # Filtra las URL para los enlaces internos de Wikipedia\n",
    "    enlaces_wikipedia = [enlace for enlace in enlaces if enlace.startswith('/wiki/')]\n",
    "\n",
    "    # Imprime los enlaces\n",
    "    for enlace in enlaces_wikipedia:\n",
    "        print(enlace)\n",
    "else:\n",
    "    print('Error al obtener la página:', html.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontró el elemento con id='usctitlechanged'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://uscode.house.gov/download/download.shtml' \n",
    "\n",
    "html = requests.get(url)\n",
    "\n",
    "if html.status_code == 200:\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "    # Encuentra el elemento que contiene la información sobre los cambios en el título.\n",
    "    div_cambios = soup.find('div', id='usctitlechanged')\n",
    "\n",
    "    if div_cambios is not None:\n",
    "        # Extrae el texto del elemento\n",
    "        texto_cambios = div_cambios.get_text()\n",
    "\n",
    "        # Analiza el texto para determinar la cantidad de títulos que han cambiado\n",
    "        titulos_cambiados = len(texto_cambios.split(','))\n",
    "\n",
    "        # Imprime la cantidad de títulos cambiados\n",
    "        print(\"Número de títulos cambiados:\", titulos_cambiados)\n",
    "    else:\n",
    "        print(\"No se encontró el elemento con id='usctitlechanged'\")\n",
    "else:\n",
    "    print('Error al obtener la página:', html.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An official website of the United States government\n",
      "Ten Most Wanted Fugitives\n",
      "Fugitives\n",
      "Capitol Violence\n",
      "Terrorism\n",
      "Kidnappings & Missing Persons\n",
      "Parental Kidnappings\n",
      "Seeking Information\n",
      "Indian Country\n",
      "ECAP\n",
      "ViCAP\n",
      "Bank Robbers\n",
      "Ten Most Wanted Fugitives FAQ\n",
      "Ten Most Wanted Fugitives Historical Pictures\n",
      "Most Wanted\n",
      "Ten Most Wanted\n",
      "Fugitives\n",
      "Terrorism\n",
      "Kidnappings / Missing Persons\n",
      "Seeking Information\n",
      "Bank Robbers\n",
      "ECAP\n",
      "ViCAP\n",
      "FBI Jobs\n",
      "Submit a Tip\n",
      "Crime Statistics\n",
      "History\n",
      "FOIPA\n",
      "Scams & Safety\n",
      "FBI Kids\n",
      "FBI Tour\n",
      "News\n",
      "Stories\n",
      "Videos\n",
      "Press Releases\n",
      "Speeches\n",
      "Testimony\n",
      "Podcasts and Radio\n",
      "Photos\n",
      "Español\n",
      "Apps\n",
      "How We Can Help You\n",
      "Law Enforcement\n",
      "Victims\n",
      "Parents and Caregivers\n",
      "Students\n",
      "Businesses\n",
      "Safety Resources\n",
      "Need an FBI Service or More Information?\n",
      "What We Investigate\n",
      "Terrorism\n",
      "Counterintelligence\n",
      "Cyber Crime\n",
      "Public Corruption\n",
      "Civil Rights\n",
      "Organized Crime\n",
      "White-Collar Crime\n",
      "Violent Crime\n",
      "WMD\n",
      "About\n",
      "Mission & Priorities\n",
      "Leadership & Structure\n",
      "Partnerships\n",
      "Community Outreach\n",
      "FAQs\n",
      "Contact Us\n",
      "Field Offices\n",
      "FBI Headquarters\n",
      "Overseas Offices\n",
      "Accessibility\n",
      "eRulemaking\n",
      "Freedom of Information / Privacy Act\n",
      "Legal Notices\n",
      "Legal Policies & Disclaimers\n",
      "Privacy Policy\n",
      "USA.gov\n",
      "White House\n",
      "No FEAR Act\n",
      "Equal Opportunity\n",
      "FBI.gov\n",
      "Sign-up for email updates\n",
      "Accessibility\n",
      "eRulemaking\n",
      "Freedom of Information / Privacy Act\n",
      "Legal Notices\n",
      "Legal Policies & Disclaimers\n",
      "Privacy Policy\n",
      "USA.gov\n",
      "White House\n",
      "No FEAR Act\n",
      "Equal Opportunity\n",
      "FBI Laboratory Quality System Documents\n"
     ]
    }
   ],
   "source": [
    "html = requests.get(url)\n",
    "\n",
    "if html.status_code == 200:\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "    # Encuentra todos los enlaces directamente dentro del HTML utilizando soup.find_all('a')\n",
    "    enlaces = soup.find_all('a')\n",
    "\n",
    "    # Extrae los nombres de los enlaces que tienen el atributo 'title' y los guarda en una lista llamada nombres_mas_buscados\n",
    "    nombres_mas_buscados = [enlace.get('title') for enlace in enlaces if enlace.get('title') is not None]\n",
    "\n",
    "    # Imprime la lista de nombres\n",
    "    for nombre in nombres_mas_buscados:\n",
    "        print(nombre)\n",
    "else:\n",
    "    print('Error al obtener la página:', html.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Fecha, Hora, Latitud, Longitud, Región]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "\n",
    "html = requests.get(url)\n",
    "\n",
    "if html.status_code == 200:\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "    # Encuentra la tabla que contiene la información de los terremotos\n",
    "    tabla=soup.find('table', class_='eqs table-scroll') \n",
    "\n",
    "     # Obtiene todas las filas de la tabla excluyendo la primera fila de encabezado\n",
    "    filas = tabla.find_all('tr')[1:21]  # 20 filas para los 20 terremotos más recientes\n",
    "\n",
    "    # Crea listas vacías para almacenar los datos\n",
    "    fechas = []\n",
    "    horas = []\n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "    regiones = []\n",
    "\n",
    "    # Recorre cada fila de la tabla y extrae los datos necesarios\n",
    "    for fila in filas:\n",
    "        columna_fecha_hora = fila.find('td', class_='tabev6').text.strip().split('\\xa0')\n",
    "        fecha = columna_fecha_hora[0]\n",
    "        hora = columna_fecha_hora[1]\n",
    "        coordenadas = fila.find('td', class_='tabev1').text.strip()\n",
    "        latitud = coordenadas.split()[0]\n",
    "        longitud = coordenadas.split()[1]\n",
    "        region = fila.find('td', class_='tb_region').text.strip()\n",
    "\n",
    "        # Agrega los datos extraídos a las listas correspondientes\n",
    "        fechas.append(fecha)\n",
    "        horas.append(hora)\n",
    "        latitudes.append(latitud)\n",
    "        longitudes.append(longitud)\n",
    "        regiones.append(region)\n",
    "\n",
    "    # Crea un diccionario con los datos\n",
    "    datos = {\n",
    "        'Fecha': fechas,\n",
    "        'Hora': horas,\n",
    "        'Latitud': latitudes,\n",
    "        'Longitud': longitudes,\n",
    "        'Región': regiones\n",
    "    }\n",
    "\n",
    "    # Crea el dataframe de pandas con los datos\n",
    "    dataframe = pd.DataFrame(datos)\n",
    "\n",
    "    print(dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m countries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Iterate over each row in the table (excluding the header row)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Extract the data from the columns\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     date \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart-date\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     26\u001b[0m     day \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://hackevents.co/hackathons'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table that contains the hackathon information\n",
    "table = soup.find('table')\n",
    "\n",
    "# Create empty lists to store the data\n",
    "dates = []\n",
    "days = []\n",
    "titles = []\n",
    "cities = []\n",
    "countries = []\n",
    "\n",
    "# Iterate over each row in the table (excluding the header row)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    # Extract the data from the columns\n",
    "    date = row.find('td', class_='start-date').text.strip()\n",
    "    day = row.find('span', class_='date').text.strip()\n",
    "    title = row.find('a').text.strip()\n",
    "    city = row.find('td', class_='city').text.strip()\n",
    "    country = row.find('td', class_='country').text.strip()\n",
    "\n",
    "    # Append the data to the respective lists\n",
    "    dates.append(date)\n",
    "    days.append(day)\n",
    "    titles.append(title)\n",
    "    cities.append(city)\n",
    "    countries.append(country)\n",
    "\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    'Date': dates,\n",
    "    'Day': days,\n",
    "    'Title': titles,\n",
    "    'City': cities,\n",
    "    'Country': countries\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head(25)\n",
    "\n",
    "# NO ENCUENTRA LA TABLA, IGUAL HAY UN PROBLEMA DE ACTUALIZACIÓN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "#Ten en cuenta que necesitarás tener una cuenta de desarrollador de Twitter y las credenciales de la API para acceder a la \n",
    "#API de Twitter mediante tweepy. Además, ten en cuenta que hay un límite de solicitudes a la API de Twitter, así que ten \n",
    "#cuidado al hacer muchas consultas seguidas.\n",
    "\n",
    "# Agrega tus credenciales de la API de Twitter\n",
    "#eemplazar TU_CONSUMER_KEY, TU_CONSUMER_SECRET, TU_ACCESS_TOKEN, TU_ACCESS_TOKEN_SECRET y NOMBRE_DE_LA_CUENTA con tus propias \n",
    "#credenciales de la API de Twitter y el nombre de la cuenta de Twitter que deseas contar los tweets.\n",
    "consumer_key = 'TU_CONSUMER_KEY'\n",
    "consumer_secret = 'TU_CONSUMER_SECRET'\n",
    "access_token = 'TU_ACCESS_TOKEN'\n",
    "access_token_secret = 'TU_ACCESS_TOKEN_SECRET'\n",
    "\n",
    "# Configura las credenciales de autenticación\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Crea una instancia de la API\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Nombre de la cuenta de Twitter que deseas contar los tweets\n",
    "nombre_cuenta = 'NOMBRE_DE_LA_CUENTA'\n",
    "\n",
    "try:\n",
    "    # Obtiene el objeto del usuario\n",
    "    usuario = api.get_user(screen_name=nombre_cuenta)\n",
    "    \n",
    "    # Obtiene el número de tweets del usuario\n",
    "    num_tweets = usuario.statuses_count\n",
    "    \n",
    "    print(f\"El número de tweets de @{nombre_cuenta} es: {num_tweets}\")\n",
    "    \n",
    "    #NO TENGO CUENTA DE TWITER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Agrega tus credenciales de la API de Twitter\n",
    "consumer_key = 'TU_CONSUMER_KEY'\n",
    "consumer_secret = 'TU_CONSUMER_SECRET'\n",
    "access_token = 'TU_ACCESS_TOKEN'\n",
    "access_token_secret = 'TU_ACCESS_TOKEN_SECRET'\n",
    "\n",
    "# Configura las credenciales de autenticación\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Crea una instancia de la API\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Nombre de la cuenta de Twitter de la que deseas contar los seguidores\n",
    "nombre_cuenta = 'NOMBRE_DE_LA_CUENTA'\n",
    "\n",
    "try:\n",
    "    # Obtiene el objeto del usuario\n",
    "    usuario = api.get_user(screen_name=nombre_cuenta)\n",
    "    \n",
    "    # Obtiene el número de seguidores del usuario\n",
    "    num_seguidores = usuario.followers_count\n",
    "    \n",
    "    print(f\"El número de seguidores de @{nombre_cuenta} es: {num_seguidores}\")\n",
    "\n",
    "    \n",
    "    #NO TENGO CUENTA DE TIWTER\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 6 691 000+\n",
      "日本語: 1 382 000+\n",
      "Español: 1 881 000+\n",
      "Русский: 1 930 000+\n",
      "Deutsch: 2 822 000+\n",
      "Français: 2 540 000+\n",
      "Italiano: 1 820 000+\n",
      "中文: 1 369 000+\n",
      "Português: 1 105 000+\n",
      "فارسی: فارسی\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL para realizar el web scraping\n",
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "​\n",
    "# URL para realizar el web scraping\n",
    "url = 'https://www.wikipedia.org/'\n",
    "​\n",
    "# Realizar una solicitud GET a la URL\n",
    "response = requests.get(url)\n",
    "​\n",
    "# Crear un objeto BeautifulSoup para analizar el contenido HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "​\n",
    "# Encontrar todos los elementos con la clase 'central-featured-lang'\n",
    "langs = soup.find_all('div', class_='central-featured-lang')\n",
    "​\n",
    "# Recorrer los elementos y extraer el nombre del idioma y el número de artículos relacionados\n",
    "for lang in langs:\n",
    "    lang_name = lang.find('strong').text\n",
    "    article_count = lang.find('bdi').text\n",
    "    print(f'{lang_name}: {article_count}')\n",
    "English: 6 691 000+\n",
    "日本語: 1 382 000+\n",
    "Español: 1 881 000+\n",
    "Русский: 1 930 000+\n",
    "Deutsch: 2 822 000+\n",
    "Français: 2 540 000+\n",
    "Italiano: 1 820 000+\n",
    "中文: 1 369 000+\n",
    "Português: 1 105 000+\n",
    "فارسی: فارسی\n",
    "Top 10 languages by number of native speakers stored in a Pandas Dataframe\n",
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "​\n",
    "# URL para realizar el web scraping\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "​\n",
    "# Realizar una solicitud GET a la URL\n",
    "response = requests.get(url)\n",
    "​\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "​\n",
    "​\n",
    "table = soup.find('table', class_='wikitable sortable')\n",
    "​\n",
    "​\n",
    "rankings = []\n",
    "languages = []\n",
    "native_speakers = []\n",
    "​\n",
    "# Iterar sobre las filas de la tabla (excluyendo la primera fila de encabezado)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    # Extraer los datos de las columnas\n",
    "    columns = row.find_all('td')\n",
    "    rank = columns[0].text.strip()\n",
    "    language = columns[1].text.strip()\n",
    "    speakers = columns[2].text.strip()\n",
    "​\n",
    "    # Agregar los datos a las listas\n",
    "    rankings.append(rank)\n",
    "    languages.append(language)\n",
    "    native_speakers.append(speakers)\n",
    "​\n",
    "# Crear un diccionario con los datos\n",
    "data = {\n",
    "    'Rank': rankings,\n",
    "    'Language': languages,\n",
    "    'Native Speakers': native_speakers\n",
    "}\n",
    "​\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "​\n",
    "# Convertir el número de hablantes nativos a formato numérico\n",
    "df['Native Speakers'] = df['Native Speakers'].str.replace(',', '').astype(int)\n",
    "​\n",
    "# Ordenar el DataFrame por el número de hablantes nativos de manera descendente\n",
    "df = df.sort_values(by='Native Speakers', ascending=False)\n",
    "​\n",
    "# Obtener los 10 idiomas principales\n",
    "top_10 = df.head(10)\n",
    "​\n",
    "# Mostrar el DataFrame\n",
    "top_10.reset_index(drop=True, inplace=True)\n",
    "top_10\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "url = 'https://www.wikipedia.org/'\n",
    "\n",
    "# Realizar una solicitud GET a la URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Crear un objeto BeautifulSoup para analizar el contenido HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Encontrar todos los elementos con la clase 'central-featured-lang'\n",
    "langs = soup.find_all('div', class_='central-featured-lang')\n",
    "\n",
    "# Recorrer los elementos y extraer el nombre del idioma y el número de artículos relacionados\n",
    "for lang in langs:\n",
    "    lang_name = lang.find('strong').text\n",
    "    article_count = lang.find('bdi').text\n",
    "    print(f'{lang_name}: {article_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Native Speakers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>12.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Hindi</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Russian</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Western Punjabi</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank          Language  Native Speakers\n",
       "0    1  Mandarin Chinese             12.3\n",
       "1    2           Spanish              6.0\n",
       "2    3           English              5.1\n",
       "3    3            Arabic              5.1\n",
       "4    5             Hindi              3.5\n",
       "5    6           Bengali              3.3\n",
       "6    7        Portuguese              3.0\n",
       "7    8           Russian              2.1\n",
       "8    9          Japanese              1.7\n",
       "9   10   Western Punjabi              1.3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL para realizar el web scraping\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "\n",
    "# Realizar una solicitud GET a la URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Crear un objeto BeautifulSoup para analizar el contenido HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Encontrar la tabla que contiene la información\n",
    "table = soup.find('table', class_='wikitable sortable')\n",
    "\n",
    "# Crear listas vacías para almacenar los datos\n",
    "rankings = []\n",
    "languages = []\n",
    "native_speakers = []\n",
    "\n",
    "# Iterar sobre las filas de la tabla (excluyendo la primera fila de encabezado)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    # Extraer los datos de las columnas\n",
    "    columns = row.find_all('td')\n",
    "    rank = columns[0].text.strip()\n",
    "    language = columns[1].text.strip()\n",
    "    speakers = columns[2].text.strip().rstrip('%')  # Eliminar el símbolo \"%\" al final de la cadena\n",
    "\n",
    "    # Agregar los datos a las listas\n",
    "    rankings.append(rank)\n",
    "    languages.append(language)\n",
    "    native_speakers.append(speakers)\n",
    "\n",
    "# Crear un diccionario con los datos\n",
    "data = {\n",
    "    'Rank': rankings,\n",
    "    'Language': languages,\n",
    "    'Native Speakers': native_speakers\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convertir el número de hablantes nativos a formato numérico\n",
    "df['Native Speakers'] = df['Native Speakers'].str.replace(',', '').astype(float)\n",
    "\n",
    "# Ordenar el DataFrame por el número de hablantes nativos de manera descendente\n",
    "df = df.sort_values(by='Native Speakers', ascending=False)\n",
    "\n",
    "# Obtener los 10 idiomas principales\n",
    "top_10 = df.head(10)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "top_10.reset_index(drop=True, inplace=True)\n",
    "top_10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
